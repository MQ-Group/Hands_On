 - 工具与资源
   - 使用指南
     - [模拟器使用方法](simulators.md) - Ramulator2、Olive 等体系结构模拟器
     - [lm_eval_harness 使用方法](lm_eval_harness.md) - 大模型评估框架

    **可量化的数据：**
    - 精度
    - xx工艺/xx综合工具下的面积
    - FPGA资源消耗

    **难以（或只能粗略）量化的数据：**
    - 加速比（包括 Throughput、Latency）
    - 系统开销（Energy Breakdown，PTPX 工具会更准一些）

    ## 使用需求

    直接使用已有的 simulator 时，方法需很好地适配实际需求。很多工作是在已有 simulator 基础上进行改造，或者直接自研一个 simulator。在对比过程中，常常会将他人的方法用自己实现的（手搓的）simulator复现，以获得对比数据。

    这也是为什么 Evaluation 部分的性能数据绝大多数以**柱状图**形式呈现，只有相对值而缺乏绝对量化数据。即使与 CPU/GPU 对比时，也通常不考虑这些通用平台上可用的优化手段，数据大概率仅供趋势参考。

    **核心建议：** 重点在于自己的方法**逻辑清晰、言之有理、与设计高度契合**。

      **simulator配置：**
        python实现的simulator一般比较简洁，依赖也少。但C++尤其是早期的一些simulator，配置复杂，最好踩过一次坑后使用docker打包，供自己或别人使用。

   - Simulator
     - Simulator 原理
       - 概念级/数学级评估
         - 用最简单的公式估算执行时间：
         - $$\text{Execution Time} \approx \frac{\text{Total MACs}}{\text{Throughput}}$$。
         - 其中 Total MACs (总乘加次数): 这是一个静态的模型属性，可以通过分析网络结构（卷积层、全连接层的权重和输入维度）精确计算得出。
         - Throughput 直接按乘法器数量 × 频率估算（理想满载），例如，一个拥有 N 个乘法器、工作在 f GHz 的处理器，其理论吞吐率就是 N × f Giga MACs/second。特点是功耗与延迟数据都相当爆炸，如果对比这类工作需要用相同评估方法，否则没有可比性。
                - 补充说明
         - 这类模型本质是一种基于理论峰值的性能估算模型（Performance Estimation Model），处于最高度的抽象层次。

       - 细化的性能模型（逼近时钟级）
         - 将模型分解为卷积/GEMM/激活等算子，分别建模计算与访存，考虑片上缓存命中、带宽与延迟、事件/拓扑调度等；常用 Python 进行原型、C++ 实现高性能与底层细节。
         - 对每个操作建立更精细的性能模型：
           - 计算时间: 基于 MACs，考虑不同数据类型（如 INT8 vs FP32）的计算单元差异。
           - 访存时间: 依据输入/权重/输出数据量与内存带宽、延迟建模估算数据搬运时间。
           - 片上缓存: 引入命中/缺失模型，命中则极短，未命中则需承受 DRAM 高延迟。
           - 事件调度: 将执行视为 DAG，操作需等待所有前驱完成后才能开始。
         - 高精度内存模拟（如 Ramulator）: 以 [JEDEC](https://www.jedec.org/)(包含各种规格存储器标准数据) 官方数据为准，精确实现 DRAM 状态机与时序（tCAS/tRCD/tRP…），在离散时钟周期内推进命令与资源冲突，常与 gem5 等全系统模拟器集成（用于CPU/SOC仿真）。
         - 统一思想: 将系统抽象为大型状态机，按离散时间步（如时钟周期）依据规则由当前状态推演至下一状态。

       - 细化的性能模型 (逼近时钟级精度) 补充
         - 为提升精度，需要将更多硬件与模型细节纳入考量，从单一 MAC 计数演进至事件/时钟驱动模型。
         - 原理
           - 模型分解: 将复杂计算任务分解为卷积、矩阵乘、激活、池化等更小操作。
           - 分部建模: 对每个操作建立更精细的模型，考虑计算/访存/缓存/调度等。
             - 计算时间: 依然基于 MACs，考虑数据类型差异。
             - 访存时间: 结合带宽与延迟建模输入/权重/输出的数据搬运时间。
             - 片上缓存: 建立命中/缺失模型。
             - 事件调度: 按 DAG 拓扑顺序执行。
         - 实现语言
           - Python: 适合上层模型快速搭建与验证，便于描述计算图与数据流，便于快速迭代调度策略与硬件参数。
           - C++: 适合高性能与底层细节建模，如地址计算与缓存替换策略，实现高性能。
         - 特点
           - 优点: 精度较高，能捕捉访存行为对性能的关键影响，指导数据排布与循环优化。
           - 缺点: 开发复杂度高，需深入体系结构知识；模拟速度较慢。
       - 高精度、调用官方数据的复杂模拟器 (如 Ramulator)
         - 这是模拟器谱系中最复杂与精确的一类，聚焦关键部件的逼真模拟。
         - Ramulator 案例分析
           - 专注领域: 模拟现代 DRAM 系统行为，作为内存子系统的精细化工具，并非完整系统模拟器。
           - 原理: 精确实现 DRAM 标准（DDR3/DDR4/LPDDR4/HBM）的状态机与时序参数。
           - DRAM 结构建模: 建模通道（Channel）/列（Rank）/片（Bank）/行（Row）/列（Column）。
           - 命令与时序: 模拟 ACTIVATE/PRECHARGE/READ/WRITE 等命令并遵守 JEDEC 大量时序约束（tCAS、tRCD、tRP、tRAS 等），命令可发出取决于 Bank 状态与相关时序是否满足。
           - 调用官方数据: 直接使用厂商 Datasheet 时序参数配置模拟核心，保证与真实硬件行为一致。
           - 整合与使用: 通过标准接口集成进全系统模拟器（如 gem5、ZSim），CPU 访存请求交由 Ramulator 进行精确时序模拟，按计算出的正确时钟周期返回结果。
         - 特点
           - 优点: 精度极高，能复现 Bank/Row 冲突导致的性能下降，可用于研究调度算法与评估不同 DRAM 类型。
           - 缺点: 模拟速度慢、使用与配置复杂，需要深厚体系结构背景。

